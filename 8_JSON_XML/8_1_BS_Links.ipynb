{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python для анализа данных\n",
    "\n",
    "## Веб-скрэйпинг: переход по ссылкам, скачивание файлов\n",
    "\n",
    "*Автор: Татьяна Рогович, НИУ ВШЭ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы с вами работали с табличными данными. Второй очень частой задачей для скрэйпинга является автоматический переход по ссылкам. Обычно мы встречаемся с двумя сценариями: переход по нумерованным страницам (обычно это выдача поиска или некоторый упорядоченный архив документов) и переход по определенным ссылкам на странице. Сегодня будем учиться делать и то, и то.\n",
    "\n",
    "Давайте начнем с учебного примера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "По ссылке http://py4e-data.dr-chuck.net/known_by_Lilian.html список людей, которых знает Лилиан. Нужно найти ее 18-го друга (считаем с одного) и перейти по ссылке (там будет список людей, которых знает этот друг). Какой имя вы извлечете, если эту операцию повторить 7 раз? То есть нам нужно найти 18-го друга 6-го человека.\n",
    "\n",
    "Прежде всего изучите исходный код страницы. В каком теге лежат ссылки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://py4e-data.dr-chuck.net/known_by_Tigan.html\">Tigan</a>\n"
     ]
    }
   ],
   "source": [
    "# решение\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "friends = requests.get('http://py4e-data.dr-chuck.net/known_by_Lilian.html').text\n",
    "soup = BeautifulSoup(friends, 'lxml')\n",
    "print(soup.find_all('a')[17]) # ссылки лежат в тэге 'a', находим 18-го друга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что нужная нам информация лежит в атрибуте href, достать текст, как мы делали раньше не поможет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Tigan.html\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('a')[17].get('href')) # с помощью метода get достаем информацию из атрибут href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось упаковать все в цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Tigan.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mickey.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Marvin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Yago.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Daood.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jillian.html\n",
      "Ответ: Bradly\n"
     ]
    }
   ],
   "source": [
    "friends = requests.get('http://py4e-data.dr-chuck.net/known_by_Lilian.html').text\n",
    "soup = BeautifulSoup(friends, 'lxml')\n",
    "\n",
    "for ix in range(6):\n",
    "    link = soup.find_all('a')[17].get('href')\n",
    "    print(link)\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "    \n",
    "print('Ответ: '+soup.find_all('a')[17].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачивание файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, еще одно применение скрэйпинга, о котором мы пока не поговорили - это автоматизированное скачивание файлов. Например, вы хотите скачать научные статьи по определенному ключевому слову или прайс-листы поставщика, которые он загружает на сайт в эскеле.\n",
    "\n",
    "Давайте посмотрим, как скачивать файлы на примере pdf и заодно попробуем походить по ссылкам. Кстати, этот процесс еще часто называется spidering или crawling, потому что ваш скрипт как паучок ползет по ссылкам (отсюда и название первых поисковых роботов - spider).\n",
    "\n",
    "Давайте попробуем сохранить дипломные работы выпускников факультета компьютерных наук ВШЭ.\n",
    "\n",
    "Мы уже отредактировали фильтры и ссылка их запомнила. Позже сегодня посмотрим как можно самим заполнять такие поля с помощью Selenium.\n",
    "\n",
    "https://www.hse.ru/edu/vkr/?faculty=120026365&author=&supervisor=&year=&title=&level=0&rating=&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2&lang=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.hse.ru/edu/vkr/?faculty=120026365&author=&supervisor=&year=&title=&level=0&rating=&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2&lang=en'\n",
    "soup = BeautifulSoup(requests.get(link).text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала поэкспериментируем с первым студентом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"https://www.hse.ru/cookie.html\" target=\"_blank\" title=\"Пройти по ссылке\">здесь</a>,\n",
       " <a class=\"link\" href=\"https://www.hse.ru/data_protection_regulation\" target=\"_blank\" title=\"Пройти по ссылке\">здесь</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-spb\" href=\"//spb.hse.ru/\">Санкт-Петербург</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-nn\" href=\"//nnov.hse.ru/\">Нижний Новгород</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-perm\" href=\"https://perm.hse.ru/\">Пермь</a>,\n",
       " <a class=\"link link_no-visited link_no-underline\" href=\"/en/edu/vkr/\">EN</a>,\n",
       " <a class=\"link link_dark no-visited\" href=\"//www.hse.ru/search/search.html?simple=0&amp;searchid=2284688\">Расширенный поиск по сайту</a>,\n",
       " <a class=\"link no-visited header_breadcrumb__link\" href=\"//www.hse.ru\"><span>Национальный исследовательский университет «Высшая школа экономики»</span></a>,\n",
       " <a class=\"link\" href=\"/edu/vkr/296302309\">Disparity Estimation via Neural Networks</a>,\n",
       " <a class=\"link\" href=\"/staff/akonushin\">Конушин Антон Сергеевич</a>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(soup.find_all('a', {'class':'link'})))\n",
    "soup.find_all('a', {'class':'link'})[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что ссылок очень много, а нам нужны только те, которые ведут на дипломные работы. Поиграем в детектива и обнаружим, что нужные нам ссылки выглядят следующим образом: 'edu/vkr/296302309'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link link_no-visited link_no-underline\" href=\"/en/edu/vkr/\">EN</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296302309\">Disparity Estimation via Neural Networks</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296305514\">Empirical Study of Deep Neural Network Loss Surfaces</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296303935\">Multiple-choice Question Answering in the Russian Language</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296279411\">Communication Complexity and its Applications</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296306052\">Deep Learning Approaches for Classification Subcellular Protein Patterns in Human Cells</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296306124\">Sequential Learning of Sparse ResNet Blocks</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/296306016\">Human Pose Estimation With Deep Structured Models</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/219430191\">Text Embeddings for Recommender Systems)</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/219398831\">Generators of Words Definitions</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/219430466\">Learning Spacecraft Control with Reinforcement Learning</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/index.html?lang=en&amp;page=2&amp;faculty=120026365&amp;program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&amp;textAvailable=2\">2</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/index.html?lang=en&amp;page=3&amp;faculty=120026365&amp;program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&amp;textAvailable=2\">3</a>\n",
      "<a class=\"link\" href=\"/edu/vkr/index.html?lang=en&amp;page=2&amp;faculty=120026365&amp;program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&amp;textAvailable=2\">→</a>\n",
      "<a class=\"link\" href=\"https://www.hse.ru/edu/vkr/\">Расширенный поиск ВКР</a>\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    if 'edu/vkr' in link.get('href'):\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ага, уже лучше, но видим, что еще остались \"лишние\" ссылки. Зато видим, что во всех нужных нам после слэша цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/edu/vkr/206745575\n",
      "/edu/vkr/182863926\n",
      "/edu/vkr/182864188\n",
      "/edu/vkr/182948750\n",
      "/edu/vkr/183280945\n",
      "/edu/vkr/182948863\n",
      "/edu/vkr/183155936\n",
      "/edu/vkr/182647987\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if len(re.findall(r'/edu/vkr/\\d+', link.get('href'))):\n",
    "        vkr_link = re.findall(r'/edu/vkr/\\d+', link.get('href'))[0]\n",
    "        print(vkr_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь давайте разберемся с первым студентом, а потом упакуем все в цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    if len(re.findall(r'/edu/vkr/\\d+', link.get('href'))):\n",
    "        vkr_link = re.findall(r'/edu/vkr/\\d+', link.get('href'))[0]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = 'https://www.hse.ru' + vkr_link # в ссылке, которую мы скачали, не хранится домен, добавим его\n",
    "student_soup = BeautifulSoup(requests.get(student).text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Играем в детективов на самой странице и понимаем, что нам нужна ссылка, в которой есть текст \"Текст работы\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link\" href=\"http://lms.hse.ru/ap_service.php?getwork=1&amp;guid=6288C78F-42A8-4F36-9439-D09B05EE3384\">Текст работы</a>\n",
      "http://lms.hse.ru/ap_service.php?getwork=1&guid=6288C78F-42A8-4F36-9439-D09B05EE3384\n"
     ]
    }
   ],
   "source": [
    "for x in student_soup.find_all('a'):\n",
    "    if x.text == 'Текст работы':\n",
    "        print(x)\n",
    "        file_link = x.get('href')\n",
    "        print(file_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде бы нашли. Теперь попробуем сохранить файл. Файлы студенты загружают в разных форматах, давайте скачивать только pdf. Иногда название файла будет прямо в ссылке, но это, увы, не наш случай. Будем проверять, что лежит по ссылке с помощью метода headers, который показывает, что находится в ответе по запросу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/pdf'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = requests.get(file_link, stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "if course.headers['content-type'] == 'application/pdf':\n",
    "    fh = open('test.pdf', 'wb') # wb - запись байтовой информации\n",
    "    fh.write(course.content) # считываем туда \"содержание\" файла по ссылке\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталась одна проблема, что при запуске в цикле было бы неплохо автоматически генерировать имя файла. Давайте забирать из тэга `a` название работы, когда мы находим ссылку на ВКР. Давайте соберем все вместе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.hse.ru/edu/vkr/?faculty=120026365&author=&supervisor=&year=&title=&level=0&rating=&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2&lang=en'\n",
    "soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if len(re.findall(r'/edu/vkr/\\d+', link.get('href'))):\n",
    "        vkr_link = re.findall(r'/edu/vkr/\\d+', link.get('href'))[0]\n",
    "        vkr_title = link.text\n",
    "        \n",
    "        student = 'https://www.hse.ru' + vkr_link\n",
    "        student_soup = BeautifulSoup(requests.get(student).text, 'lxml')\n",
    "        \n",
    "        for x in student_soup.find_all('a'):\n",
    "            if x.text == 'Текст работы':\n",
    "                file_link = x.get('href')\n",
    "                \n",
    "        course = requests.get(file_link, stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "        if course.headers['content-type'] == 'application/pdf':\n",
    "            fh = open(vkr_title+'.pdf', \"wb\") # создаем файл\n",
    "            fh.write(course.content) # считываем туда \"содержание\" файла по ссылке\n",
    "            fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось! И последнее. У нас было 28 работ на 3 страницах, давайте по страницам тоже пройдемся. Кликните в браузере на страницу 2 по ссылке, а потом обратно на 1. Обратите внимание, что ссылка в браузере поменялась и теперь в ней появился параметр page. Скопируем новую ссылку для первой страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1 # создаю переменную и с помощью форматирования строк добавляю ее в ссылку после page=\n",
    "link = f'https://www.hse.ru/edu/vkr/index.html?lang=en&page={page}&faculty=120026365&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.hse.ru/edu/vkr/index.html?lang=en&page=1&faculty=120026365&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link # проверили, что работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,4):\n",
    "    link = f'https://www.hse.ru/edu/vkr/index.html?lang=en&page={page}&faculty=120026365&program=g122722324%3Bg134854782%3Bp135181588%3Bp220156607%3Bp135181634%3Bg122392182%3Bp135181636&textAvailable=2'\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        if len(re.findall(r'/edu/vkr/\\d+', link.get('href'))):\n",
    "            vkr_link = re.findall(r'/edu/vkr/\\d+', link.get('href'))[0]\n",
    "            vkr_title = link.text\n",
    "\n",
    "            student = 'https://www.hse.ru' + vkr_link\n",
    "            student_soup = BeautifulSoup(requests.get(student).text, 'lxml')\n",
    "\n",
    "            for x in student_soup.find_all('a'):\n",
    "                if x.text == 'Текст работы':\n",
    "                    file_link = x.get('href')\n",
    "\n",
    "            course = requests.get(file_link, stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "            if course.headers['content-type'] == 'application/pdf':\n",
    "                fh = open(vkr_title+'.pdf', \"wb\") # создаем файл\n",
    "                fh.write(course.content) # считываем туда \"содержание\" файла по ссылке\n",
    "                fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
